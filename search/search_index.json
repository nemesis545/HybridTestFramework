{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"HybridTestFramework \u00b6 Architecture \u00b6 In the era of cloud-native world we cannot stick to a particular framework, however due to projects requirement we often need to evolve the existing testing solution in such a way so that it can cater multiple testing requirement, hence HybridTestFramework is targeting to create a bridge between the kind of legacy systems or the systems which are still in a transition phase of migrate to cloud with super cool cloud-native systems. Also, it's worth to mention as we are trying to follow the pattern of testing pyramid where the testing is more focused for the api followed by WebUI, in future the framework focus will be more towards the apis and events. Framework Capabilities \u00b6 Cross browser testing support. Added browserstack support for CrossBrowser testing. Running tests in docker containers selenium grid. Running tests in AWS DeviceFarm selenium grid. Running tests in selenium server in docker containers. Security testing using OWASP, running in docker container. Rest Api and GraphQL testing support powered by RestAssured. gRPC api testing support using native gRPC=java library. Event driven microservice testing based on pubsub model. Support for Kafka, Cloud Pubsub, AWS SNS testing and continue evolving. Visual regression testing using percy.io. Accessibility testing using axe-selenium. Stubbed api testing using WireMock. Can send logs to ElasticSearch for kibana dashboard visualization. Database testing support. Kubernetes support. GitHub actions execution \u00b6 JenkinsExecution \u00b6 Azure devops TestResults \u00b6 Allure Reporting \u00b6 BrowserStack Dashboard \u00b6","title":"Home"},{"location":"#hybridtestframework","text":"","title":"HybridTestFramework"},{"location":"#architecture","text":"In the era of cloud-native world we cannot stick to a particular framework, however due to projects requirement we often need to evolve the existing testing solution in such a way so that it can cater multiple testing requirement, hence HybridTestFramework is targeting to create a bridge between the kind of legacy systems or the systems which are still in a transition phase of migrate to cloud with super cool cloud-native systems. Also, it's worth to mention as we are trying to follow the pattern of testing pyramid where the testing is more focused for the api followed by WebUI, in future the framework focus will be more towards the apis and events.","title":"Architecture"},{"location":"#framework-capabilities","text":"Cross browser testing support. Added browserstack support for CrossBrowser testing. Running tests in docker containers selenium grid. Running tests in AWS DeviceFarm selenium grid. Running tests in selenium server in docker containers. Security testing using OWASP, running in docker container. Rest Api and GraphQL testing support powered by RestAssured. gRPC api testing support using native gRPC=java library. Event driven microservice testing based on pubsub model. Support for Kafka, Cloud Pubsub, AWS SNS testing and continue evolving. Visual regression testing using percy.io. Accessibility testing using axe-selenium. Stubbed api testing using WireMock. Can send logs to ElasticSearch for kibana dashboard visualization. Database testing support. Kubernetes support.","title":"Framework Capabilities"},{"location":"#github-actions-execution","text":"","title":"GitHub actions execution"},{"location":"#jenkinsexecution","text":"","title":"JenkinsExecution"},{"location":"#azure-devops-testresults","text":"","title":"Azure devops TestResults"},{"location":"#allure-reporting","text":"","title":"Allure Reporting"},{"location":"#browserstack-dashboard","text":"","title":"BrowserStack Dashboard"},{"location":"api/","text":"Api Testing \u00b6 Write your first user journey api test \u00b6 Create new class and name as the TC00*_E2E_TEST-*** Provide jira link in @Link Provide all the api components as @Feature Provide test severity and description Write test Use CatchBlock in try/catch section Create pojo for the request body and deserialize the response. By using the project lombok it's easy to create the data model. @Data @AllArgsConstructor @NoArgsConstructor @Jacksonized public class Booking { private String firstname ; private String lastname ; private int totalprice ; private boolean depositpaid ; private Bookingdates bookingdates ; private String additionalneeds ; } @Severity ( SeverityLevel . CRITICAL ) @Test ( description = \"E2E test for Trading Coins\" ) @Description ( \"Get Trading Coins\" ) @Story ( \"Test CryptoCoins\" ) public void TestTradings (){ setBaseURI ( \"https://api.coingecko.com\" ); Response response = httpGet ( \"/api/v3/search/trending\" ); Assert . assertEquals ( getStatusCode ( response ) /*actual value*/ , 200 /*expected value*/ , \"Correct status code returned\" ); Trades trades = response . getBody (). as ( Trades . class ); Assert . assertNotNull ( trades . getCoins (). get ( 0 ). item . name ); Assert . assertNotNull ( trades . getCoins (). get ( 0 ). item . slug ); }","title":"Apis"},{"location":"api/#api-testing","text":"","title":"Api Testing"},{"location":"api/#write-your-first-user-journey-api-test","text":"Create new class and name as the TC00*_E2E_TEST-*** Provide jira link in @Link Provide all the api components as @Feature Provide test severity and description Write test Use CatchBlock in try/catch section Create pojo for the request body and deserialize the response. By using the project lombok it's easy to create the data model. @Data @AllArgsConstructor @NoArgsConstructor @Jacksonized public class Booking { private String firstname ; private String lastname ; private int totalprice ; private boolean depositpaid ; private Bookingdates bookingdates ; private String additionalneeds ; } @Severity ( SeverityLevel . CRITICAL ) @Test ( description = \"E2E test for Trading Coins\" ) @Description ( \"Get Trading Coins\" ) @Story ( \"Test CryptoCoins\" ) public void TestTradings (){ setBaseURI ( \"https://api.coingecko.com\" ); Response response = httpGet ( \"/api/v3/search/trending\" ); Assert . assertEquals ( getStatusCode ( response ) /*actual value*/ , 200 /*expected value*/ , \"Correct status code returned\" ); Trades trades = response . getBody (). as ( Trades . class ); Assert . assertNotNull ( trades . getCoins (). get ( 0 ). item . name ); Assert . assertNotNull ( trades . getCoins (). get ( 0 ). item . slug ); }","title":"Write your first user journey api test"},{"location":"aws/","text":"AWS Cloud Testing \u00b6","title":"AWS"},{"location":"aws/#aws-cloud-testing","text":"","title":"AWS Cloud Testing"},{"location":"graphql/","text":"GraphQL api Testing \u00b6 Write your first user journey graph api test \u00b6 Create new graph api test by using the following test conventions @Severity ( SeverityLevel . NORMAL ) @Test ( description = \"E2E test for graphql\" ) @Description ( \"Get Fruit Shop\" ) @Story ( \"Test Graphql\" ) public void TestFruitShop () { String query = \"query{\\n\" + \" products(id: \\\"7\\\") {\\n\" + \" name\\n\" + \" price\\n\" + \" category {\\n\" + \" name\\n\" + \" }\\n\" + \" vendor {\\n\" + \" name\\n\" + \" id\\n\" + \" }\\n\" + \" }\\n\" + \"}\" ; String jsonString = graphqlToJson ( query ); setBaseURI ( \"https://www.predic8.de/fruit-shop-graphql?\" ); RestAssured . given () . contentType ( \"application/json\" ) . body ( jsonString ) . when (). post (). then () . assertThat () . statusLine ( \"HTTP/1.1 200 OK\" ) . log () . body (); }","title":"GraphQL"},{"location":"graphql/#graphql-api-testing","text":"","title":"GraphQL api Testing"},{"location":"graphql/#write-your-first-user-journey-graph-api-test","text":"Create new graph api test by using the following test conventions @Severity ( SeverityLevel . NORMAL ) @Test ( description = \"E2E test for graphql\" ) @Description ( \"Get Fruit Shop\" ) @Story ( \"Test Graphql\" ) public void TestFruitShop () { String query = \"query{\\n\" + \" products(id: \\\"7\\\") {\\n\" + \" name\\n\" + \" price\\n\" + \" category {\\n\" + \" name\\n\" + \" }\\n\" + \" vendor {\\n\" + \" name\\n\" + \" id\\n\" + \" }\\n\" + \" }\\n\" + \"}\" ; String jsonString = graphqlToJson ( query ); setBaseURI ( \"https://www.predic8.de/fruit-shop-graphql?\" ); RestAssured . given () . contentType ( \"application/json\" ) . body ( jsonString ) . when (). post (). then () . assertThat () . statusLine ( \"HTTP/1.1 200 OK\" ) . log () . body (); }","title":"Write your first user journey graph api test"},{"location":"grpc/","text":"gRPC testing \u00b6 Write your first gRPC testing \u00b6 First get the proto file from the server and put it in contract\\proto folder then the compilation will auto generate the required code. syntax = \"proto3\" ; package greet ; option go_package = \"greetpb\" ; option java_multiple_files = true ; option java_outer_classname = \"GreetProto\" ; option java_package = \"com.greet\" ; service GreetService { // Unary rpc Greet ( GreetRequest ) returns ( GreetResponse ) {}; } message Greeting { string first_name = 1 ; string last_name = 2 ; } message GreetRequest { Greeting greeting = 1 ; } message GreetResponse { string result = 1 ; } Create the client code for the rpc connection, then use the client to write the test CoffeeGrpc . CoffeeBlockingStub coffeeServiceStub ; public CoffeeClient () throws IOException { ManagedChannel channel = channel ( \"coffee-service-i6avjiaelq-ts.a.run.app\" , ChannelType . TLS , AuthType . TLS ); coffeeServiceStub = CoffeeGrpc . newBlockingStub ( channel ); } public AddCoffeeResponse addCoffee () throws Exception { try { AddCoffeeRequest addCoffeeRequest = AddCoffeeRequest . newBuilder () . setId ( UUID . randomUUID (). toString ()) . setName ( \"java\" ) . setFlavour ( \"Java Grpc\" ) . setAroma ( \"Hybrid\" ) . setCoffeeSize ( CoffeeSize . SMALL ) . setDescription ( \"New java coffee client\" ) . setLastUpdated ( Timestamp . newBuilder (). build ()) . build (); return coffeeServiceStub . addCoffee ( addCoffeeRequest ); } catch ( Exception e ) { return null ; } } private final CoffeeClient coffeeClient ; public TC009_GrpcApi () throws IOException { coffeeClient = new CoffeeClient (); } @Test public void updateCoffee () throws Exception { UpdateCoffeeResponse bookResponse = coffeeClient . updateCoffee (); Assert . assertEquals ( bookResponse . getMessage (), \"Coffee Details Updated\" ); }","title":"gRPC"},{"location":"grpc/#grpc-testing","text":"","title":"gRPC testing"},{"location":"grpc/#write-your-first-grpc-testing","text":"First get the proto file from the server and put it in contract\\proto folder then the compilation will auto generate the required code. syntax = \"proto3\" ; package greet ; option go_package = \"greetpb\" ; option java_multiple_files = true ; option java_outer_classname = \"GreetProto\" ; option java_package = \"com.greet\" ; service GreetService { // Unary rpc Greet ( GreetRequest ) returns ( GreetResponse ) {}; } message Greeting { string first_name = 1 ; string last_name = 2 ; } message GreetRequest { Greeting greeting = 1 ; } message GreetResponse { string result = 1 ; } Create the client code for the rpc connection, then use the client to write the test CoffeeGrpc . CoffeeBlockingStub coffeeServiceStub ; public CoffeeClient () throws IOException { ManagedChannel channel = channel ( \"coffee-service-i6avjiaelq-ts.a.run.app\" , ChannelType . TLS , AuthType . TLS ); coffeeServiceStub = CoffeeGrpc . newBlockingStub ( channel ); } public AddCoffeeResponse addCoffee () throws Exception { try { AddCoffeeRequest addCoffeeRequest = AddCoffeeRequest . newBuilder () . setId ( UUID . randomUUID (). toString ()) . setName ( \"java\" ) . setFlavour ( \"Java Grpc\" ) . setAroma ( \"Hybrid\" ) . setCoffeeSize ( CoffeeSize . SMALL ) . setDescription ( \"New java coffee client\" ) . setLastUpdated ( Timestamp . newBuilder (). build ()) . build (); return coffeeServiceStub . addCoffee ( addCoffeeRequest ); } catch ( Exception e ) { return null ; } } private final CoffeeClient coffeeClient ; public TC009_GrpcApi () throws IOException { coffeeClient = new CoffeeClient (); } @Test public void updateCoffee () throws Exception { UpdateCoffeeResponse bookResponse = coffeeClient . updateCoffee (); Assert . assertEquals ( bookResponse . getMessage (), \"Coffee Details Updated\" ); }","title":"Write your first gRPC testing"},{"location":"kafka/","text":"Kafka Testing \u00b6 What is Apache Kafka? \u00b6 Apache Kafka is a framework implementation of a software bus using stream-processing. It is an open-source software platform developed by the Apache Software Foundation written in Scala and Java. The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. Behind the scenes, Kafka is distributed, scales well, replicates data across brokers (servers), can survive broker downtime, and much more. Topics, Partitions and Offsets \u00b6 Topics: A particular stream of data Similar to a table of the database You can have as many topics you can A topic is identified by its name Topics are split in partitions Each partition is ordered Each message in partition will get an incremental ID called offset Partition 0, 1, 2 .... Order only guaranteed within a partition, not across partitions Data is kept only for a limited time. Once the data is written to a partition it cannot be changed. Example Scenario : You can have multiple cabs, and each cabs reports its GPS location to kafka. You can have a topic cabs_gps that contains the position of all cabs. Each cab will send a message to kafka every 20 sec, each message will contain the cabID, and the cab location(lat/long) Brokers & Topics \u00b6 A kafka cluster is composed of multiple brokers(servers) Each broker is identified by its ID(integer) Each broker contains certain topic partitions After connecting to any broker(called a bootstrap broker), you will be connected to the entire cluster A good number to get start is 3 brokers, but some big clusters have more than 100 brokers Example of topic A with 3 partitions Example of topic B with 2 partitions Topics replication \u00b6 Topics should have a replication factor >1 (Usually between 2 and 3) This way if one broker is down another broker can serve the data. Example of topic A with replication factor 2 At any time only ONE broker can be a leader for a given partition Only that leader can receive and serve data for a partition. The other broker will synchronize the data. So each partition has one leader and multiple ISR (in-sync-replica) Producer \u00b6 Producer write data to topics(which is made of partitions) Producer automatically know to which broker and partition to write. In case broker failure, Producers will automatically recover Producers can choose to receive acknowledgment of data writes. acks=0 Producer won't wait for acknowledgment (Possible data loss) acks=1 Producer will wait for leader acknowledgment (Limited data loss) acks=2 Leader & Replica acknowledgment (no data loss) Producer can choose to send a key with the message(string,num etc.) If key==null data will sent round robin(broker 101 then 102 then 103) If key is sent then all message for that key will send to same partition A key is sent if we need a message ordering for a specific field as cabID. Consumer \u00b6 Consumer read data from a topic(identified by name) Consumer knows which broker to read from In case of broker failure, consumer know how to recover Data is read in order with in each partition Consumer read data in consumer groups Each consumer within a group reads form exclusive partitions If you have more consumers than partitions, some consumers will be inactive Kafka stores the offset at which a consumer group has been reading The offsets committed live in a kafka topic named _consumer_offsets When a consumer in a group has processed the data received from kafka, it should be committing the offsets. If a consumer dies, it will be able to read back from where it left off. Zookeeper \u00b6 Zookeeper manager brokers(keeps a list of them) Zookeeper helps in performing leader election for partition Zookeeper send notifications to kafka in case of any changes. Schema Registry \u00b6 Kafka takes bytes as an input and publishes them No data verification Schema registry rejects bat data A common data format must be agreed upon Apache avro as data format Data is fully typed Date is compressed automatically Schema comes along with the data Documentation is embedded in the schema Data can be read across any language Schema can be evolved over time in safe manner Avro \u00b6 Apache Avro is a data serialization system. * Avro provides: * Rich data structures. * A compact, fast, binary data format. * A container file, to store persistent data. * Remote procedure call (RPC). * Simple integration with dynamic languages. Code generation is not required to read or write data files nor to use or implement RPC protocols. Code generation as an optional optimization, only worth implementing for statically typed languages. {\"namespace\": \"dip.avro\", \"type\": \"record\", \"name\": \"User\", \"fields\": [ {\"name\": \"name\", \"type\": \"string\"}, {\"name\": \"favorite_number\", \"type\": [\"int\", \"null\"]}, {\"name\": \"favorite_color\", \"type\": [\"string\", \"null\"]} ] } * Common Fields: * Name: Name of the schema * Namespace: (equivalent of package in java) * Doc: Documentation to explain your schema * Aliases: Optional other name for schema * Fields * Name: Name of field * Doc: Documentation for that field * Type: Data type for that field * Default: Default value for that field * Complex types: * Enums { \"type\": \"enum\", \"name\": \"Customer Status\", \"symbols\": [\"BRONZE\",\"SILVER\",\"GOLD\"] } * Arrays { \"type\": \"array\", \"items\": \"string\" } * Maps { \"type\": \"map\", \"values\": \"string\" } * Unions { \"name\": \"middle_name\", \"type\": [ \"null\", \"string\" ], \"default\": \"null\" } * Calling other schema as type Kafka Rest Proxy \u00b6 kafka is great for java based consumers/producers Avro support for some languages isn't great, where JSON/HTTP requests are great. Reporting data to Kafka from any frontend app built in any language not supported by official Confluent clients Ingesting messages into a stream processing framework that doesn\u2019t yet support Kafka Perform a comprehensive set of administrative operations through REST APIs, including: Describe, list, and configure brokers Create, delete, describe, list, and configure topics Delete, describe, and list consumer groups Create, delete, describe, and list ACLs List partition reassignments","title":"Kafka"},{"location":"kafka/#kafka-testing","text":"","title":"Kafka Testing"},{"location":"kafka/#what-is-apache-kafka","text":"Apache Kafka is a framework implementation of a software bus using stream-processing. It is an open-source software platform developed by the Apache Software Foundation written in Scala and Java. The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. Behind the scenes, Kafka is distributed, scales well, replicates data across brokers (servers), can survive broker downtime, and much more.","title":"What is Apache Kafka?"},{"location":"kafka/#topics-partitions-and-offsets","text":"Topics: A particular stream of data Similar to a table of the database You can have as many topics you can A topic is identified by its name Topics are split in partitions Each partition is ordered Each message in partition will get an incremental ID called offset Partition 0, 1, 2 .... Order only guaranteed within a partition, not across partitions Data is kept only for a limited time. Once the data is written to a partition it cannot be changed. Example Scenario : You can have multiple cabs, and each cabs reports its GPS location to kafka. You can have a topic cabs_gps that contains the position of all cabs. Each cab will send a message to kafka every 20 sec, each message will contain the cabID, and the cab location(lat/long)","title":"Topics, Partitions and Offsets"},{"location":"kafka/#brokers-topics","text":"A kafka cluster is composed of multiple brokers(servers) Each broker is identified by its ID(integer) Each broker contains certain topic partitions After connecting to any broker(called a bootstrap broker), you will be connected to the entire cluster A good number to get start is 3 brokers, but some big clusters have more than 100 brokers Example of topic A with 3 partitions Example of topic B with 2 partitions","title":"Brokers &amp; Topics"},{"location":"kafka/#topics-replication","text":"Topics should have a replication factor >1 (Usually between 2 and 3) This way if one broker is down another broker can serve the data. Example of topic A with replication factor 2 At any time only ONE broker can be a leader for a given partition Only that leader can receive and serve data for a partition. The other broker will synchronize the data. So each partition has one leader and multiple ISR (in-sync-replica)","title":"Topics replication"},{"location":"kafka/#producer","text":"Producer write data to topics(which is made of partitions) Producer automatically know to which broker and partition to write. In case broker failure, Producers will automatically recover Producers can choose to receive acknowledgment of data writes. acks=0 Producer won't wait for acknowledgment (Possible data loss) acks=1 Producer will wait for leader acknowledgment (Limited data loss) acks=2 Leader & Replica acknowledgment (no data loss) Producer can choose to send a key with the message(string,num etc.) If key==null data will sent round robin(broker 101 then 102 then 103) If key is sent then all message for that key will send to same partition A key is sent if we need a message ordering for a specific field as cabID.","title":"Producer"},{"location":"kafka/#consumer","text":"Consumer read data from a topic(identified by name) Consumer knows which broker to read from In case of broker failure, consumer know how to recover Data is read in order with in each partition Consumer read data in consumer groups Each consumer within a group reads form exclusive partitions If you have more consumers than partitions, some consumers will be inactive Kafka stores the offset at which a consumer group has been reading The offsets committed live in a kafka topic named _consumer_offsets When a consumer in a group has processed the data received from kafka, it should be committing the offsets. If a consumer dies, it will be able to read back from where it left off.","title":"Consumer"},{"location":"kafka/#zookeeper","text":"Zookeeper manager brokers(keeps a list of them) Zookeeper helps in performing leader election for partition Zookeeper send notifications to kafka in case of any changes.","title":"Zookeeper"},{"location":"kafka/#schema-registry","text":"Kafka takes bytes as an input and publishes them No data verification Schema registry rejects bat data A common data format must be agreed upon Apache avro as data format Data is fully typed Date is compressed automatically Schema comes along with the data Documentation is embedded in the schema Data can be read across any language Schema can be evolved over time in safe manner","title":"Schema Registry"},{"location":"kafka/#avro","text":"Apache Avro is a data serialization system. * Avro provides: * Rich data structures. * A compact, fast, binary data format. * A container file, to store persistent data. * Remote procedure call (RPC). * Simple integration with dynamic languages. Code generation is not required to read or write data files nor to use or implement RPC protocols. Code generation as an optional optimization, only worth implementing for statically typed languages. {\"namespace\": \"dip.avro\", \"type\": \"record\", \"name\": \"User\", \"fields\": [ {\"name\": \"name\", \"type\": \"string\"}, {\"name\": \"favorite_number\", \"type\": [\"int\", \"null\"]}, {\"name\": \"favorite_color\", \"type\": [\"string\", \"null\"]} ] } * Common Fields: * Name: Name of the schema * Namespace: (equivalent of package in java) * Doc: Documentation to explain your schema * Aliases: Optional other name for schema * Fields * Name: Name of field * Doc: Documentation for that field * Type: Data type for that field * Default: Default value for that field * Complex types: * Enums { \"type\": \"enum\", \"name\": \"Customer Status\", \"symbols\": [\"BRONZE\",\"SILVER\",\"GOLD\"] } * Arrays { \"type\": \"array\", \"items\": \"string\" } * Maps { \"type\": \"map\", \"values\": \"string\" } * Unions { \"name\": \"middle_name\", \"type\": [ \"null\", \"string\" ], \"default\": \"null\" } * Calling other schema as type","title":"Avro"},{"location":"kafka/#kafka-rest-proxy","text":"kafka is great for java based consumers/producers Avro support for some languages isn't great, where JSON/HTTP requests are great. Reporting data to Kafka from any frontend app built in any language not supported by official Confluent clients Ingesting messages into a stream processing framework that doesn\u2019t yet support Kafka Perform a comprehensive set of administrative operations through REST APIs, including: Describe, list, and configure brokers Create, delete, describe, list, and configure topics Delete, describe, and list consumer groups Create, delete, describe, and list ACLs List partition reassignments","title":"Kafka Rest Proxy"},{"location":"selenium/","text":"Selenium Testing \u00b6 Write your first user journey web ui test \u00b6 Create new class and name as the TC00*_E2E_TEST-*** Provide jira link in @Link Provide all the api components as @Feature Provide test severity and description Write test Use CatchBlock in try/catch section","title":"Selenium"},{"location":"selenium/#selenium-testing","text":"","title":"Selenium Testing"},{"location":"selenium/#write-your-first-user-journey-web-ui-test","text":"Create new class and name as the TC00*_E2E_TEST-*** Provide jira link in @Link Provide all the api components as @Feature Provide test severity and description Write test Use CatchBlock in try/catch section","title":"Write your first user journey web ui test"},{"location":"setup/","text":"Project Setup \u00b6 Install IntelliJ IDEA https://www.jetbrains.com/idea/download/ Install docker desktop https://www.docker.com/products/docker-desktop Java JDK_11 https://adoptium.net/temurin/archive Gradle https://gradle.org/next-steps/?version=6.8.3&format=bin Allure https://github.com/allure-framework/allure2/archive/2.17.3.zip Set Environment variables JAVA_HOME: Pointing to the Java SDK folder\\bin GRADLE_HOME: Pointing to Gradle directory\\bin. ALLURE_HOME: Pointing to allure directory\\bin. Java 11 JDK Installation and config \u00b6 Download Java 11 JDK from here Install downloaded Java 11 JDK. System Properties -> Environment Variables -> System Variables -> New -> JAVA_HOME and Value as C:\\Program Files\\Eclipse Adoptium\\jdk-11.0.13.8-hotspot (path where JDK is installed) System Properties -> Environment Variables -> System Variables -> Select Path and Edit -> Add %JAVA_HOME%\\bin Once Environment variables are configured, open command prompt and run java -v and echo $JAVA_HOME to check whether java version installed is returned. e.g $ java -version $ openjdk 11.0.13 2021-10-19<br/> OpenJDK Runtime Environment Temurin-11.0.13+8 (build 11.0.13+8)<br/> OpenJDK 64-Bit Server VM Temurin-11.0.13+8 (build 11.0.13+8, mixed mode) Recommended IDE \u00b6 Please install Community Edition of IntelliJ IDEA from here Getting Started \u00b6 ```shell script $ git clone $ cd $ import project from intellij as a gradle project $ gradle clean $ gradle build $ gradle task E2E $ gradle allureReport $ gradle allureServe ### Spawns chrome, firefox, selenium hub and OWASP proxy server ```shell script $ docker-compose up -d Complete infrastructure creation for local run \u00b6 ```shell script $ $ docker-compose -f docker-compose-infra up -d ### Spawns four additional node-chrome/firefox instances linked to the hub ```shell script $ docker-compose scale chrome=5 $ docker-compose scale firefox=5 Error Handle for dynamic classpath error in intellij: Search and modify the below line in .idea workspace.xml <component name= \"PropertiesComponent\" > <property name= \"dynamic.classpath\" value= \"true\" /> </component> Security ZAP Testing \u00b6 OWASP ZAP Download it from Github Run it Configure proxy: Tools -> Options -> Local Proxies. Set port to 8888 Get API key from your ZAP instance: Tools -> Options -> API Vulnerable application - system under test Install docker and run docker service Run bodgeit docker container (or any app) Make sure it's running on http://localhost:8080/bodgeit/ Selenium traffic will go through ZAP proxy in order to capture all traffic. It's not exactly necessary for the bodgeit shop, but in real-world applications spider would struggle to find URLs requiring logged in access.","title":"Setup"},{"location":"setup/#project-setup","text":"Install IntelliJ IDEA https://www.jetbrains.com/idea/download/ Install docker desktop https://www.docker.com/products/docker-desktop Java JDK_11 https://adoptium.net/temurin/archive Gradle https://gradle.org/next-steps/?version=6.8.3&format=bin Allure https://github.com/allure-framework/allure2/archive/2.17.3.zip Set Environment variables JAVA_HOME: Pointing to the Java SDK folder\\bin GRADLE_HOME: Pointing to Gradle directory\\bin. ALLURE_HOME: Pointing to allure directory\\bin.","title":"Project Setup"},{"location":"setup/#java-11-jdk-installation-and-config","text":"Download Java 11 JDK from here Install downloaded Java 11 JDK. System Properties -> Environment Variables -> System Variables -> New -> JAVA_HOME and Value as C:\\Program Files\\Eclipse Adoptium\\jdk-11.0.13.8-hotspot (path where JDK is installed) System Properties -> Environment Variables -> System Variables -> Select Path and Edit -> Add %JAVA_HOME%\\bin Once Environment variables are configured, open command prompt and run java -v and echo $JAVA_HOME to check whether java version installed is returned. e.g $ java -version $ openjdk 11.0.13 2021-10-19<br/> OpenJDK Runtime Environment Temurin-11.0.13+8 (build 11.0.13+8)<br/> OpenJDK 64-Bit Server VM Temurin-11.0.13+8 (build 11.0.13+8, mixed mode)","title":"Java 11 JDK Installation and config"},{"location":"setup/#recommended-ide","text":"Please install Community Edition of IntelliJ IDEA from here","title":"Recommended IDE"},{"location":"setup/#getting-started","text":"```shell script $ git clone $ cd $ import project from intellij as a gradle project $ gradle clean $ gradle build $ gradle task E2E $ gradle allureReport $ gradle allureServe ### Spawns chrome, firefox, selenium hub and OWASP proxy server ```shell script $ docker-compose up -d","title":"Getting Started"},{"location":"setup/#complete-infrastructure-creation-for-local-run","text":"```shell script $ $ docker-compose -f docker-compose-infra up -d ### Spawns four additional node-chrome/firefox instances linked to the hub ```shell script $ docker-compose scale chrome=5 $ docker-compose scale firefox=5 Error Handle for dynamic classpath error in intellij: Search and modify the below line in .idea workspace.xml <component name= \"PropertiesComponent\" > <property name= \"dynamic.classpath\" value= \"true\" /> </component>","title":"Complete infrastructure creation for local run"},{"location":"setup/#security-zap-testing","text":"OWASP ZAP Download it from Github Run it Configure proxy: Tools -> Options -> Local Proxies. Set port to 8888 Get API key from your ZAP instance: Tools -> Options -> API Vulnerable application - system under test Install docker and run docker service Run bodgeit docker container (or any app) Make sure it's running on http://localhost:8080/bodgeit/ Selenium traffic will go through ZAP proxy in order to capture all traffic. It's not exactly necessary for the bodgeit shop, but in real-world applications spider would struggle to find URLs requiring logged in access.","title":"Security ZAP Testing"}]}